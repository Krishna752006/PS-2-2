However, the predominance of closed-source models has restricted
extensive research and development.

DeepSeek-Coder series, a range of open-source code models with sizes from 1.3B to 33B, trained from scratch on 2
trillion tokens, sourced from 87 programming languages, ensuring a comprehensive
understanding of coding languages and syntax.

These models have the potential to automate and streamline
many aspects of coding, from bug detection to code generation, thereby enhancing productivity
and reducing the likelihood of human error.

The giant closed-source models, while powerful, are often inaccessible to many researchers and
developers due to their proprietary nature.
In response to this challenge, we present the DeepSeek-Coder series

also incorporated the Fill-In-Middle (FIM) approach

 DeepSeek-Coder-Base 7B demonstrates competitive performance when compared to models that are five times larger, such as CodeLlama-33B


Our analysis rigorously examines the impact of FIM training strategies on the pretraining
phase of code models. . The outcomes of these comprehensive studies shed light on intriguing
aspects of FIM configurations, offering valuable insights that significantly contribute to the
enhancement and development of code pretrained models.

The training dataset of DeepSeek-Coder is composed of 87% source code, 10% English coderelated natural language corpus, and 3% code-unrelated Chinese natural language corpus.

This process involves data crawling, rule-based filtering, dependency parsing, repositorylevel deduplication, and quality screening

To reduce the amount of data to be processed, we
apply filtering rules similar to those used in the StarCoder project (Li et al., 2023) to preliminarily
filter out lower-quality code. By applying these filtering rules, we reduce the total amount of
data to only 32.8% of its original size.

. To make the paper self-contained, we briefly describe the
filter rules used in the StarCoder Data project:
Firstly, we filter out files with an average line length exceeding 100 characters or a maximum
line length surpassing 1000 characters. Additionally, we remove files with fewer than 25%
alphabetic characters. Except for the XSLT programming language, we further filter out files
where the string "<?xml version=" appeared in the first 100 characters. For HTML files, we
consider the ratio of visible text to HTML code. We retain files where the visible text constitutes
at least 20% of the code and is no less than 100 characters. For JSON and YAML files, which
typically contain more data, we only keep files that have a character count ranging from 50 to
5000 characters. This effectively removes most data-heavy files.

large language models for code are mainly pre-trained on file-level source code, which ignores
the dependencies between different files in a project. However, in practical applications, such
models struggle to effectively scale to handle entire project-level code scenarios.
will consider how to leverage the dependencies between files within the same repository in
this step

The algorithm 1 describes a topological sort for dependency analysis

significant performance improvements that can be
achieved by deduplicating training datasets for Large Language Models (LLMs)

performance of LLMs can be enhanced by removing long repetitive substrings

near-deduplication is a crucial preprocessing step for
achieving competitive performance on code benchmark tasks

deduplication at the repository level of code, rather than at the file
level, as the latter approach may filter out certain files within a repository, potentially disrupting
the structure of the repository

employ a compiler
and a quality model, combined with heuristic rules, to further filter out low-quality data.

The total data volume is 798 GB with
603 million files.

 To ensure that our code training data is not contaminated by information
from the test set, which may be present on GitHub, we’ve implemented an n-gram filtering
process. This process involves the removal of any code segments that match specific criteria.

Specifically, we filter out files containing docstrings, questions, and solutions from sources

we apply the following rules: if a
piece of code includes a 10-gram string identical to any in the test data, it is excluded from our
training data. In cases where the test data comprises strings that are shorter than 10-grams but
no less than 3-grams, we use an exact match approach for filtering.

The first training objective for our model is known as next token prediction. In this process,
various files are concatenated to form a fixed-length entry. Then, these entries are used to train
the model, enabling it to predict the subsequent token based on the provided context.

The second training objective for our model is known as fill-in-the-middle.

Due to specific dependencies in a programming language, relying
solely on next token prediction is insufficient to learn this fill-in-the-middle capability. Therefore,
several approaches (Bavarian et al., 2022; Li et al., 2023) propose the pretraining method of
Fill-in-the-Midlle (FIM). 

This approach involves randomly dividing the text into three parts,
then shuffling the order of these parts and connecting them with special characters. This method
aims to incorporate a fill-in-the-blank pretraining task during the training process. Within the
FIM methodology, two distinct modes are employed: PSM (Prefix-Suffix-Middle) and SPM
(Suffix-Prefix-Middle).

In the PSM mode, the training corpus is organized in the sequence
of 𝑃𝑟𝑒 𝑓 𝑖𝑥, 𝑆𝑢 𝑓 𝑓 𝑖𝑥, 𝑀𝑖𝑑𝑑𝑙𝑒, aligning the text in a way that the middle segment is flanked by the
prefix and suffix. Conversely, the SPM mode arranges the segments as 𝑆𝑢 𝑓 𝑓 𝑖𝑥, 𝑃𝑟𝑒 𝑓 𝑖𝑥, 𝑀𝑖𝑑𝑑𝑙𝑒,
presenting a different structural challenge.

Our primary objective was to assess the efficacy of the Fill-in-the-Middle
(FIM) technique, utilizing the HumanEval-FIM benchmark

....

For the tokenization process, we employ the HuggingFace Tokenizer library
vocabulary size of 32,000

Model Architecture
We develop a range of models with varying parameters to cater to diverse applications, including models with 1.3B, 6.7B, and 33B parameters. These models are built upon the same framework as the DeepSeek Large Language Model (LLM) outlined by DeepSeek-AI (2024). Each model is a decoder-only Transformer, incorporating Rotary Position Embedding (RoPE) as described by Suet al. (2023). Notably, the DeepSeek 33B model integrates Grouped-Query-Attention (GQA) with a group size of 8, enhancing both training and inference efficiency. Additionally, we employ FlashAttention v2 (Dao, 2023) to expedite the computation involved in the attention mechanism. The architectural details of our models are summarized in Table 2.

Following DeepSeek LLM (DeepSeek-AI, 2024), we use AdamW (Loshchilov and Hutter, 2019)
as the optimizer with 𝛽1 and 𝛽2 values of 0.9 and 0.95. We adapt batch sizes and learning rates by
the scaling laws suggested in DeepSeek LLM. For the learning rate scheduling, we implement a
three-stage policy, which includes 2000 warm-up steps, and set the final learning rate to 10% of
the initial rate. Notably, the learning rate at each stage is scaled down to √︃1/ √︃10 of the preceding
stage’s rate, following the guidelines established in DeepSeek LLM (DeepSeek-AI, 2024).

Our experiments are conducted using the HAI-LLM (High-Flyer, 2023) framework, known for
its efficiency and lightweight approach in training large language models.  Our experiments
utilize clusters outfitted with NVIDIA A100 and H800 GPUs.

...

handling extended contexts, we have reconfigured the RoPE (Su et al., 2023) parameters to extend the default context window 

We develop DeepSeek-Coder-Instruct by enhancing the DeepSeek-Coder-Base through instructionbased fine-tuning using high-quality data. This data comprises helpful and impartial human instructions, structured by the Alpaca Instruction format (Taori et al., 2023).

employed a unique delimiter token <|EOT|> to signify the conclusion of each segment

we evaluate DeepSeek-Coder on four tasks, including code generation (§4.1), FIM
code completion (§4.2), cross-file code completion (§4.3) and program-based math reasoning
(§4.4)

The HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) benchmarks are widely used for evaluating code LLMs.

HumanEval consists of 164 hand-written Python problems that are validated using test cases to assess the code generated by a Code LLM in a zero-shot setting, while the MBPP benchmark includes 500 problems in a few-shot setting. To evaluate the model’s multilingual capabilities, we expanded the Python problems of Humaneval Benchmark to seven additional commonly used programming languages, namely C++, Java, PHP, TypeScript (TS), C#, Bash, and JavaScript (JS) (Cassano et al., 2023)

DeepSeek-Coder-Base achieves stateof-the-art performance with an average accuracy of 50.3% on HumanEval and 66.0% on MBPP. In comparison to the similarly sized open-source model CodeLlama-Base 34B, our model has demonstrated a notable improvement of 9% and 11% in accuracy, respectively. It’s worth noting that even our smaller model, DeepSeek-Coder-Base 6.7B, surpasses the performance of CodeLlama-Base 34B. After instruction fine-tuning, our model surpasses the closed-source GPT-3.5-Turbo model in HumanEval benchmark, significantly reducing the performance gap between OpenAI GPT-4 and open-source models.

In contrast, the DS-1000 benchmark, as introduced in the work by Lai et al. (2023), offers a comprehensive collection of 1,000 practical and realistic data science workflows across seven different libraries.

. We collected the latest problems from LeetCode Contests to prevent the appearance of both the problems or their solutions in our pre-training data. A total of 180 problems were collected from July 2023 to January 2024

Our analysis indicates that the implementation of Chain-of-Thought (CoT) prompting notably enhances the capabilities of DeepSeek-Coder-Instruct models. This improvement becomes
particularly evident in the more challenging subsets of tasks. 

By adding the directive, "You need first to write a step-by-step outline and then write the code." following the initial prompt, we have observed enhancements in performance. This observation leads us to believe that the process of first crafting detailed code descriptions assists the model in more effectively understanding and addressing the intricacies of logic and dependencies in coding tasks, particularly those of higher complexity. Therefore, we strongly recommend employing CoT prompting strategies when utilizing DeepSeek-Coder-Instruct models for complex coding challenges. Such an approach promotes a more methodical and logical framework for problem-solving, potentially resulting in more precise and efficient outcomes in code generation tasks.

DeepSeek-Coder models are trained with a 0.5 FIM (Fill-In-the-Middle) rate during their pretraining phase. This specialized training strategy empowers the model to proficiently generate
code by filling in blanks based on the surrounding context, both prefix and suffix, of the given
code snippet.

DeepSeek-Coder models are trained with a 0.5 FIM (Fill-In-the-Middle) rate during their pretraining phase. This specialized training strategy empowers the model to proficiently generate code by filling in blanks based on the surrounding context, both prefix and suffix, of the given code snippet.

We use CrossCodeEval (Ding et al., 2023) to evaluate the
capabilities of currently available open-source code models of 7B scale in cross-file completion
tasks. This dataset is constructed on a diverse set of real-world, open-sourced, permissively
licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#.


we utilize the Program-Aided Math Reasoning (PAL) method as outlined in Gao et al. (2023)
These benchmarks includes GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), GSMHard (Gao et al., 2023), SVAMP (Patel et al., 2021), TabMWP (Lu et al., 2022), ASDiv (Miao
et al., 2020) and MAWPS (Gou et al., 2023)

To further enhance the natural language understanding and mathematical reasoning abilities of the DeepSeek-Coder model, we perform additional pre-training from the general language model DeepSeek-LLM-7B Base (DeepSeek-AI, 2024) on 2 trillion tokens, resulting in DeepSeekCoder-v1.5 7B. 

Programming: This category includes evaluations in a multilingual setting using the
HumanEval dataset by Chen et al. (2021), as well as evaluations in a Python setting using
the MBPP dataset by Austin et al. (2021)

• Math Reasoning: We assess performance on math reasoning tasks using the GSM8K
benchmark (Cobbe et al., 2021) and the MATH (Hendrycks et al., 2021) benchmark [4].
These tasks involve solving math problems by generating programs.

• Natural Language Our evaluation in natural language tasks includes MMLU (Hendrycks
et al., 2020), BBH (Suzgun et al., 2022), HellaSwag (Zellers et al., 2019), Winogrande (Sakaguchi et al., 2021), and ARC-Challenge (Clark et al., 2018) benchmarks.

